{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Simulated Boycott Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"dark\")\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "NUM_USERS = 6040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: let's load our master data file (a single, aggregated csv)\n",
    "and take a glance at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13770\n",
      "6180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (300) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# old_df = pd.read_csv('all_results_v01.csv', dtype={'indices': str})\n",
    "df = pd.read_csv('all_results.csv', dtype={'indices': str})\n",
    "print(len(df.index))\n",
    "df = df[df.ratingfrac.notna()]\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply transformation that will affect the entire dataframe.\n",
    "1. Calculate the number of users included in each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(num_users_boycotting = [NUM_USERS - int(x) for x in df.num_users])\n",
    "\n",
    "# drop columns we're not going to use.\n",
    "for column in df.columns.values:\n",
    "    if 'tailndcgfull' in column:\n",
    "        df = df.drop(column, axis=1)\n",
    "\n",
    "# old_df = old_df.assign(num_users_boycotting = [NUM_USERS - int(x) for x in old_df.num_users])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the lists that can be used throughout. By editing this cell we can easily modify the full notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodval 0.7917576771996123\n",
      "goodval 0.7707049537755648\n",
      "goodval 0.9083991352840053\n",
      "goodval 0.7992590656809996\n",
      "goodval 0.7555507250557071\n",
      "goodval 0.7934699275627399\n",
      "goodval 0.7729129845829772\n",
      "goodval 0.9092223545941135\n",
      "goodval 0.8035561666164517\n",
      "goodval 0.7576885503570141\n",
      "global defaultdict(<class 'dict'>, {'KNNBaseline_item_msd': {'ndcg10': -25.206712008386738, 'tailndcg10': -24.063605217380726, 'ndcgfull': -8.934436155327113, 'prec10t4': -21.453008326226954, 'tailprec10t4': -21.802446799594133}, 'SVD': {'ndcg10': -25.368110481189, 'tailndcg10': -24.280537656644015, 'ndcgfull': -9.016887857338213, 'prec10t4': -21.87304660783151, 'tailprec10t4': -22.023081924215404}})\n",
      "movie defaultdict(<class 'dict'>, {'KNNBaseline_item_msd': {'ndcg10': -2.316653111465638, 'tailndcg10': -1.8543996226268522, 'ndcgfull': -0.607565356913951, 'prec10t4': -1.7330414237136504, 'tailprec10t4': -1.4097354325029365}, 'SVD': {'ndcg10': -2.527446665665193, 'tailndcg10': -2.134778544404378, 'ndcgfull': -0.6975562937441195, 'prec10t4': -2.258534297970622, 'tailprec10t4': -1.6879087821670382}})\n",
      "guess3 defaultdict(<class 'dict'>, {'KNNBaseline_item_msd': {'ndcg10': -25.300454148662105, 'tailndcg10': -24.039839763803027, 'ndcgfull': -8.823648910505666, 'prec10t4': -21.19621127706749, 'tailprec10t4': -21.62598230115501}, 'SVD': {'ndcg10': -25.461650332747237, 'tailndcg10': -24.256840095383687, 'ndcgfull': -8.906200920432415, 'prec10t4': -21.617622807843677, 'tailprec10t4': -21.847115322002068}})\n"
     ]
    }
   ],
   "source": [
    "id_vars = ['name','algo_name', 'indices', 'ratingfrac', 'userfrac', 'num_ratings', 'num_users', 'num_users_boycotting']\n",
    "id_vars = [x for x in id_vars if x in list(df.columns.values)]\n",
    "metrics = [\n",
    "    # 'rmse',\n",
    "    'ndcg10',\n",
    "    'tailndcg10',\n",
    "    'ndcgfull',\n",
    "    'prec10t4',\n",
    "    'tailprec10t4',\n",
    "]\n",
    "organized_experiments = [\n",
    "    'gender', 'age', 'state',\n",
    "    'occupation', 'power', 'genre'\n",
    "]\n",
    "algo_names = [\n",
    "    'KNNBaseline_item_msd',\n",
    "    'SVD'\n",
    "]\n",
    "standard_algo_names = ['KNNBaseline_item_msd', 'SVD', 'GlobalMean', 'MovieMean', 'GuessThree']\n",
    "standard_results = {}\n",
    "algo_to_metric_vs_global_mean = defaultdict(dict)\n",
    "algo_to_metric_vs_movie_mean = defaultdict(dict)\n",
    "algo_to_metric_vs_guess_three = defaultdict(dict)\n",
    "\n",
    "for algo_name in standard_algo_names:\n",
    "    filename_ratingcv_standards = 'standard_results/{}_ratingcv_standards_for_{}.json'.format(\n",
    "        'ml-1m', algo_name)\n",
    "    with open(filename_ratingcv_standards, 'r') as f:\n",
    "        standard_results[algo_name] = json.load(f)\n",
    "for main_algo_name in algo_names:\n",
    "    for metric in metrics:\n",
    "        goodval = standard_results[main_algo_name].get(metric, 0)\n",
    "        print('goodval', goodval)\n",
    "        gmean = standard_results['GlobalMean'][metric]\n",
    "        mmean = standard_results['MovieMean'].get(metric, 0)\n",
    "        guess_three = standard_results['GuessThree'][metric]\n",
    "        algo_to_metric_vs_global_mean[main_algo_name][metric] = (gmean - goodval) / goodval * 100 if goodval else 0\n",
    "        algo_to_metric_vs_movie_mean[main_algo_name][metric] = (mmean - goodval) / goodval * 100 if goodval else 0\n",
    "        algo_to_metric_vs_guess_three[main_algo_name][metric] = (guess_three - goodval) / goodval * 100 if goodval else 0\n",
    "print('global', algo_to_metric_vs_global_mean)\n",
    "print('movie', algo_to_metric_vs_movie_mean)\n",
    "print('guess3', algo_to_metric_vs_guess_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo_to_metric_to_slope = defaultdict(dict)\n",
    "algo_to_metric_to_intercept = defaultdict(dict)\n",
    "algo_to_metric_to_average_boycott = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  name  num_users  num_users_boycotting\n",
      "5416  1208 user sample     4832.0                  1208\n",
      "5418  1208 user sample     4832.0                  1208\n",
      "5419  1208 user sample     4832.0                  1208\n",
      "5421  1208 user sample     4832.0                  1208\n",
      "5422  1208 user sample     4832.0                  1208\n"
     ]
    }
   ],
   "source": [
    "samples_df = df[df['type'] == 'sample_users']\n",
    "if not samples_df.empty:\n",
    "    print(samples_df[['name', 'num_users', 'num_users_boycotting']].head())\n",
    "org_df = df[df['type'].isin(organized_experiments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "org_df.name = [\n",
    "    x.replace('excluded', '')\n",
    "    .replace('users from', '')\n",
    "    .replace('US_', '')\n",
    "    .replace('state', '')\n",
    "    .strip()\n",
    "    for x in list(org_df.name)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how well the # of ratings correlates with error.\n",
    "This will be an important piece of context for analyzing the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for algo_name in algo_names:\n",
    "#     filt = org_df[org_df.algo_name == algo_name]\n",
    "#     for metric in metrics:\n",
    "#         key = 'percent_increase_{}_non-boycott'.format(metric)\n",
    "#         sns.jointplot(filt.num_ratings, y=filt[key], kind=\"reg\").fig.suptitle(algo_name)\n",
    "#         plt.show()\n",
    "#         slope, intercept, r_value, p_value, std_err = stats.linregress(filt.num_ratings, y=filt[key])\n",
    "#         print(slope, intercept, r_value, p_value)\n",
    "#         algo_to_metric_to_slope[algo_name][metric] = slope\n",
    "#         algo_to_metric_to_intercept[algo_name][metric] = intercept\n",
    "        \n",
    "#         # calculate the average error for all boycott users in each dataframe\n",
    "#         algo_to_metric_to_average_boycott[algo_name][metric] = np.mean(\n",
    "#             samples_df[(\n",
    "#                 (samples_df.algo_name == algo_name) &\n",
    "#                 (samples_df.userfrac == 1.0) & \n",
    "#                 (samples_df.ratingfrac == 1.0)\n",
    "#             )][key.replace('non-boycott', 'boycott')]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_cols(cols, metrics, groups, percents):\n",
    "    \"\"\"take a list of cols and filter based on metrics/groups/percents\"\"\"\n",
    "    increase_cols = [\n",
    "        x for x in cols if (any(metric in x for metric in metrics) and 'increase' in x)\n",
    "    ]\n",
    "    increase_cols = [\n",
    "        x for x in increase_cols if any(group == x.split('_')[-1] for group in groups)\n",
    "    ]\n",
    "    if percents:\n",
    "        increase_cols = [x for x in increase_cols if 'percent' in x]\n",
    "    else:\n",
    "        increase_cols = [x for x in increase_cols if 'percent' not in x]\n",
    "    print(increase_cols)\n",
    "    return increase_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_in_longform(df):\n",
    "    \"\"\"Fill in a longform dataframe with metric, group, and name information\"\"\"\n",
    "    df = df.assign(\n",
    "        metric=[x.split('_')[-2] for x in df.increase_type]\n",
    "    )\n",
    "    df = df.assign(\n",
    "        group=[x.split('_')[-1] for x in df.increase_type]\n",
    "    )\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_b_curve(df, metrics, groups, percents=False, lm_plot=True, reg_plot=False):\n",
    "    algo_to_metric_to_interp = defaultdict(dict)\n",
    "    if lm_plot:\n",
    "        increase_cols = select_cols(list(df.columns.values), metrics, groups, percents)\n",
    "        longform = df[increase_cols + id_vars].melt(\n",
    "            id_vars = id_vars,\n",
    "            var_name='increase_type'\n",
    "        )\n",
    "        longform = fill_in_longform(longform)\n",
    "        \n",
    "        grid = sns.lmplot(\n",
    "            x=\"num_users_boycotting\", y=\"value\", hue=\"group\", data=longform,\n",
    "            sharey='row', sharex='col',\n",
    "            size=5, row='metric', col='algo_name',\n",
    "            fit_reg=False,\n",
    "            x_estimator=np.mean, ci=99,\n",
    "        )\n",
    "        # CALC LOG PLOT\n",
    "#         vals = [float(x) for x in list(longform.value)]\n",
    "#         minval = np.min(vals)\n",
    "#         shifted = vals + - minval + 0.00001\n",
    "#         print(np.min(shifted))\n",
    "#         longform = longform.assign(logy = np.log2(shifted))\n",
    "#         g = sns.lmplot(\n",
    "#             x=\"num_users_boycotting\", y=\"logy\", hue=\"algo_name\", data=longform,\n",
    "#             size=6, row='metric',\n",
    "#             x_estimator=np.mean,\n",
    "#             order=2\n",
    "#         )\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for algo_name in algo_names:\n",
    "            filt = df[df.algo_name == algo_name]\n",
    "            key = 'increase_{}_non-boycott'.format(metric)\n",
    "            if percents:\n",
    "                key = 'percent_' + key\n",
    "            x = filt.num_users_boycotting\n",
    "            user_nums = list(set(filt.num_users_boycotting))\n",
    "            y = filt[key]\n",
    "            \n",
    "            num_to_mean = {}\n",
    "            for num_users_boycotting in user_nums:\n",
    "                filt_by_name = filt[filt.num_users_boycotting == num_users_boycotting]\n",
    "                num_to_mean[num_users_boycotting] = np.mean(filt_by_name[key])\n",
    "            meany = np.array(list(num_to_mean.values()))\n",
    "            \n",
    "            smoothf = interp1d(user_nums, meany, kind='cubic', bounds_error=False, fill_value='extrapolate')\n",
    "            algo_to_metric_to_interp[algo_name][metric] = smoothf\n",
    "            xnew = np.linspace(min(user_nums), max(user_nums), num=500)\n",
    "            if reg_plot:\n",
    "                _, ax = plt.subplots()\n",
    "                g = sns.regplot(\n",
    "                    x=x, y=y,\n",
    "                    x_estimator=np.mean,\n",
    "                    x_bins=user_nums,\n",
    "                    ax=ax,\n",
    "                    ci=99,\n",
    "                    fit_reg=False,\n",
    "                )\n",
    "                g.set_title(algo_name)\n",
    "                \n",
    "                plt.plot(xnew, smoothf(xnew), '-')\n",
    "                plt.axhline(0, color='0.5', linestyle='--')\n",
    "                plt.axhline(algo_to_metric_vs_movie_mean[algo_name][metric], color='0.5', linestyle='--')\n",
    "    for x in grid.facet_data():\n",
    "        i_row, i_col, i_hue = x[0]\n",
    "        metric = grid.row_names[i_row]\n",
    "        algo_name = grid.col_names[i_col]\n",
    "        group = grid.hue_names[i_hue]\n",
    "        if group != 'non-boycott':\n",
    "            continue\n",
    "        # flag. \n",
    "        ax = grid.axes[i_row, i_col]\n",
    "        ax.plot(xnew, algo_to_metric_to_interp[algo_name][metric](xnew), '-', color=grid._colors[i_hue])\n",
    "        ax.axhline(0, color='0.5', linestyle='--')\n",
    "        ax.axhline(algo_to_metric_vs_movie_mean[algo_name][metric], color='0.5', linestyle='--')\n",
    "        plt.setp(ax.get_xticklabels(), visible=True, rotation=45)\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "#     for metric in metrics:\n",
    "#         for algo_name in algo_names\n",
    "\n",
    "    return algo_to_metric_to_interp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through log plots\n",
    "y = log(x)\n",
    "y = e ^ x\n",
    "ln(y) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['percent_increase_ndcg10_all', 'percent_increase_ndcg10_non-boycott', 'percent_increase_ndcgfull_all', 'percent_increase_ndcgfull_non-boycott', 'percent_increase_prec10t4_all', 'percent_increase_prec10t4_non-boycott', 'percent_increase_tailndcg10_all', 'percent_increase_tailndcg10_non-boycott', 'percent_increase_tailprec10t4_all', 'percent_increase_tailprec10t4_non-boycott']\n"
     ]
    }
   ],
   "source": [
    "algo_to_metric_to_interp = p_b_curve(\n",
    "    samples_df,\n",
    "    metrics=metrics,\n",
    "    percents=True,\n",
    "    groups=['non-boycott', 'all', ]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    for algo_name in algo_names:\n",
    "        key = 'percent_increase_{}_expected'.format(metric)\n",
    "        expected_vals = algo_to_metric_to_interp[algo_name][metric](org_df.num_users_boycotting)\n",
    "        print(key)\n",
    "        kwargs = {key: expected_vals}\n",
    "        org_df = org_df.assign(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_to_metric_vs_movie_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot2(df, metrics, groups, percents=False, kind='bar', size=10):\n",
    "    print(df.num_users_boycotting)\n",
    "    increase_cols = select_cols(list(df.columns.values), metrics, groups, percents)\n",
    "    longform = df[increase_cols + id_vars].melt(\n",
    "        id_vars = id_vars,\n",
    "        var_name='increase_type'\n",
    "    )\n",
    "    longform = fill_in_longform(longform)\n",
    "    longform = longform.assign(\n",
    "        name_plus_ratings=['{} ({}k)'.format(\n",
    "            name, int(round(num_ratings/1000,0))\n",
    "        ) for name, num_ratings in zip(longform.name, longform.num_ratings)\n",
    "    ])\n",
    "    grid = sns.factorplot(\n",
    "        x=\"value\", y=\"name\", hue=\"group\", data=longform,\n",
    "        size=size, kind=kind, col='algo_name', row='metric',\n",
    "        sharex=False,\n",
    "        row_order=metrics,\n",
    "        # linestyles='None',\n",
    "        # capsize=0.1\n",
    "    )\n",
    "    a = grid.axes\n",
    "    for x in grid.facet_data():\n",
    "        i_row, i_col, i_hue = x[0]\n",
    "        print(grid.row_names)\n",
    "        print(grid.col_names)\n",
    "        metric = grid.row_names[i_row]\n",
    "        algo_name = grid.col_names[i_col]\n",
    "        if grid.hue_names:\n",
    "            group = grid.hue_names[i_hue]\n",
    "        val = algo_to_metric_vs_movie_mean[algo_name].get(metric, 0)\n",
    "        grid.axes[i_row, i_col].axvline(0, color='0.5', linestyle='--')\n",
    "        grid.axes[i_row, i_col].axvline(val, color='0.5', linestyle='--')\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_boycotts = org_df[org_df['type'] == 'state']\n",
    "gender_boycotts = org_df[org_df['type'] == 'gender']\n",
    "age_boycotts = org_df[org_df['type'] == 'age']\n",
    "occupation_boycotts = org_df[org_df['type'] == 'occupation']\n",
    "power_boycotts = org_df[org_df['type'] == 'power']\n",
    "genre_boycotts = org_df[org_df['type'] == 'genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_users_all_ratings(df):\n",
    "    return df[(\n",
    "        (df.userfrac == 1.0) & (df.ratingfrac == 1.0)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def half_users(df):\n",
    "    return df[(\n",
    "        (df.userfrac == 0.5) & (df.ratingfrac == 1.0)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def half_ratings(df):\n",
    "    return df[(\n",
    "        (df.userfrac == 1.0) & (df.ratingfrac == 0.5)\n",
    "    )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_all_three_scenarios(df, size=6):\n",
    "    all_df = all_users_all_ratings(df)\n",
    "    if not all_df.empty:\n",
    "        plot2(\n",
    "            all_users_all_ratings(df),\n",
    "            metrics=metrics,\n",
    "            percents=True,\n",
    "            groups=['all', 'non-boycott', 'expected'],\n",
    "            size=size\n",
    "        ).fig.suptitle('All Users, All Ratings')\n",
    "    \n",
    "    half_users_df = half_users(df)\n",
    "    if not half_users_df.empty:\n",
    "        plot2(\n",
    "            half_users_df,\n",
    "            metrics=metrics,\n",
    "            percents=True,\n",
    "            groups=['all', 'non-boycott', 'boycott', 'like-boycott', 'expected'],\n",
    "            size=size\n",
    "        ).fig.suptitle('Half Users')\n",
    "        \n",
    "    half_ratings_df = half_ratings(df)\n",
    "    if not half_ratings_df.empty:\n",
    "        plot2(\n",
    "            half_ratings_df,\n",
    "            metrics=metrics,\n",
    "            percents=True,\n",
    "            groups=['non-boycott', 'boycott', 'like-boycott', 'expected'],\n",
    "            size=size,\n",
    "        ).fig.suptitle('Half Ratings')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do gender first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(gender_boycotts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States seemed uninteresting and slowed the page down a bit, so I commented out for now.\n",
    "Main conclusion was that state-based boycotts seem to be pretty ineffective for basically every state.\n",
    "One funny result was that it seems California users have especially low-value ratings - the performance degradation was quite off from the expected degradation (b/c CA has a lot of users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(state_boycotts, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the power boycotts show a very weird results. Warrants double checks.\n",
    "\n",
    "It appears for KNN, removing all power users actually improves performance (as opposed to the large expected drop).\n",
    "But at the same time, RMSE increases! This is one of the example where RMSE and NDCG do not correlate...\n",
    "\n",
    "Note the artifact in plot #5 below: it looks like ndcg has a huge increase when bottom 10% users do a half boycott... but this is b/c ndcg@10 is just getting easier to \"succeed\" for these users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(power_boycotts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(age_boycotts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(occupation_boycotts, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_three_scenarios(genre_boycotts, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at our real numbers\n",
    "Full dataset precision@10 is \n",
    "SVD: 0.8026069529047541\n",
    "KNN: 0.7999920231446164\n",
    "w/ 3000 users boycotting:\n",
    "KNN: 0.785440336\n",
    "SVD: 0.784764938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prec_w_0 = {\n",
    "    'SVD': 0.8026069529047541,\n",
    "    'KNN': 0.7999920231446164\n",
    "}\n",
    "prec_w_3000 = {\n",
    "    'SVD': 0.784764938,\n",
    "    'KNN': 0.785440336\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in ('SVD', 'KNN'):\n",
    "    old = prec_w_0[n]\n",
    "    new = prec_w_3000[n]\n",
    "    err_rate_old = (1 - old) * 10\n",
    "    err_rate_new = (1 - new) * 10\n",
    "    delta = err_rate_new - err_rate_old\n",
    "    print('err rate old vs new:', err_rate_old, err_rate_new, err_rate_new / err_rate_old)\n",
    "    print('% change err rate', delta / err_rate_old * 100)\n",
    "    percent_change = (new - old) / old * 100\n",
    "    raw_change = new - old\n",
    "    print(percent_change, '%', raw_change, 'raw')\n",
    "    err_per_user = raw_change * 10\n",
    "    print(err_per_user / err_rate_old * 100)\n",
    "    \n",
    "    err_per_1m = err_per_user * 1e6\n",
    "    users_needed_to_see_err = 1 / err_per_user\n",
    "    print('Error per user:', err_per_user)\n",
    "    print('Error per 1m users:', err_per_1m)\n",
    "    print(users_needed_to_see_err, 'users')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
